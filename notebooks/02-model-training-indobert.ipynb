{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5954f5e",
   "metadata": {},
   "source": [
    "### Environment Setup and Imports\n",
    "\n",
    "Import all required libraries and dependencies for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b26fd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a96eae",
   "metadata": {},
   "source": [
    "# IndoBERT Model Training\n",
    "\n",
    "This notebook demonstrates the fine-tuning and evaluation of IndoBERT for Indonesian tweet classification. Each section is clearly marked with headers and brief explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2f585",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Load the preprocessed train, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705418c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../dataset/processed/train.csv')\n",
    "val_df = pd.read_csv('../dataset/processed/validation.csv')\n",
    "test_df = pd.read_csv('../dataset/processed/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23d784",
   "metadata": {},
   "source": [
    "#### Data Overview\n",
    "\n",
    "Display dataset shapes and preview the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29774487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3500, 2)\n",
      "Validation shape: (750, 2)\n",
      "Test shape: (750, 2)\n",
      "\n",
      "Train DataFrame head:\n",
      "                                        cleaned_text  label\n",
      "0  warga kp bayam itu yang lahannya dipakai buat ...      5\n",
      "1  team pesona bobby kertanegara mulai turun tang...      5\n",
      "2  kapolri jenderal listyo sigit prabowo menghadi...      6\n",
      "3  ini lah hasil dari kinerja pak prabowo selama ...      5\n",
      "4  puluhan ribu masyarakat menyambut kedatangan a...      5\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"\\nTrain DataFrame head:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57481e9a",
   "metadata": {},
   "source": [
    "#### Label Distribution\n",
    "\n",
    "Analyze the distribution of labels in each dataset split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b99b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution:\n",
      "  Label 0: 43 (1.23%)\n",
      "  Label 1: 257 (7.34%)\n",
      "  Label 2: 14 (0.40%)\n",
      "  Label 3: 280 (8.00%)\n",
      "  Label 4: 280 (8.00%)\n",
      "  Label 5: 2081 (59.46%)\n",
      "  Label 6: 411 (11.74%)\n",
      "  Label 7: 134 (3.83%)\n",
      "\n",
      "Validation label distribution:\n",
      "  Label 0: 9 (1.20%)\n",
      "  Label 1: 55 (7.33%)\n",
      "  Label 2: 3 (0.40%)\n",
      "  Label 3: 60 (8.00%)\n",
      "  Label 4: 60 (8.00%)\n",
      "  Label 5: 446 (59.47%)\n",
      "  Label 6: 88 (11.73%)\n",
      "  Label 7: 29 (3.87%)\n",
      "\n",
      "Test label distribution:\n",
      "  Label 0: 10 (1.33%)\n",
      "  Label 1: 55 (7.33%)\n",
      "  Label 2: 3 (0.40%)\n",
      "  Label 3: 60 (8.00%)\n",
      "  Label 4: 60 (8.00%)\n",
      "  Label 5: 445 (59.33%)\n",
      "  Label 6: 88 (11.73%)\n",
      "  Label 7: 29 (3.87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check label distribution in train, validation, and test sets\n",
    "def print_label_distribution(df, name):\n",
    "    counts = df['label'].value_counts().sort_index()\n",
    "    percentages = counts / counts.sum() * 100\n",
    "    print(f\"{name} label distribution:\")\n",
    "    for label, count, pct in zip(counts.index, counts.values, percentages.values):\n",
    "        print(f\"  Label {label}: {count} ({pct:.2f}%)\")\n",
    "    print()\n",
    "\n",
    "print_label_distribution(train_df, \"Train\")\n",
    "print_label_distribution(val_df, \"Validation\")\n",
    "print_label_distribution(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0e662",
   "metadata": {},
   "source": [
    "### Model and Tokenizer Initialization\n",
    "\n",
    "Load the IndoBERT model and tokenizer for sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6959e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"indobenchmark/indobert-large-p2\"\n",
    "num_labels = 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b42da",
   "metadata": {},
   "source": [
    "### Dataset Preparation and Tokenization\n",
    "\n",
    "Convert data to Hugging Face Datasets and tokenize for model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Datasets and tokenize\n",
    "# Drop rows with missing cleaned_text\n",
    "train_df_clean = train_df.dropna(subset=['cleaned_text'])\n",
    "val_df_clean = val_df.dropna(subset=['cleaned_text'])\n",
    "test_df_clean = test_df.dropna(subset=['cleaned_text'])\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch['cleaned_text'], truncation=True, max_length=256)\n",
    "\n",
    "# Convert to Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df_clean[['cleaned_text', 'label']])\n",
    "val_dataset = Dataset.from_pandas(val_df_clean[['cleaned_text', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df_clean[['cleaned_text', 'label']])\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "columns = ['input_ids', 'attention_mask', 'label']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "val_dataset.set_format(type='torch', columns=columns)\n",
    "test_dataset.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a70d0",
   "metadata": {},
   "source": [
    "### Metrics, Data Collator, and Trainer Setup\n",
    "\n",
    "Define evaluation metrics, data collator, class weights, and configure the custom Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0829dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Calculate class weights for weighted loss function\n",
    "label_counts = Counter(train_df['label'])\n",
    "num_classes = len(label_counts)\n",
    "total_samples = len(train_df)\n",
    "class_weights = []\n",
    "for i in range(num_classes):\n",
    "    count = label_counts.get(i, 0)\n",
    "    if count == 0:\n",
    "        class_weights.append(0.0)\n",
    "    else:\n",
    "        class_weights.append(total_samples / (num_classes * count))\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # Move class_weights to the correct device\n",
    "        weights = class_weights.to(model.module.device if hasattr(model, 'module') else model.device)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Improved training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../logs/indobert/results_indobert\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='../logs/indobert/detailed_logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=3,\n",
    "    report_to=None,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=7)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a571d",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Train the IndoBERT model using the custom Trainer and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f04d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8eb46",
   "metadata": {},
   "source": [
    "# Model Training Results\n",
    "\n",
    "Summary of model performance and training history.\n",
    "\n",
    "The model was trained on a Kaggle Notebook to leverage GPU acceleration, which significantly speeds up the training process for large models like IndoBERT. The training was configured for 15 epochs.\n",
    "\n",
    "***\n",
    "\n",
    "### Training History\n",
    "\n",
    "The table below shows the model's performance metrics on the validation set at different steps throughout the training process.\n",
    "\n",
    "| Step | Training Loss | Validation Loss | Accuracy | F1 | Precision | Recall |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| 100 | 1.949000 | 1.949710 | 0.321762 | 0.353605 | 0.543864 | 0.321762 |\n",
    "| 200 | 1.489900 | 1.622133 | 0.514019 | 0.529652 | 0.676132 | 0.514019 |\n",
    "| 300 | 1.328500 | 1.352426 | 0.463284 | 0.467727 | 0.724507 | 0.463284 |\n",
    "| 400 | 0.714900 | 1.218028 | 0.683578 | 0.698927 | 0.769663 | 0.683578 |\n",
    "| 500 | 0.525400 | 1.333650 | 0.724967 | 0.738125 | 0.781990 | 0.724967 |\n",
    "| 600 | 0.230400 | 1.508070 | 0.781041 | 0.779882 | 0.785440 | 0.781041 |\n",
    "| 700 | 0.170600 | 1.593431 | 0.777036 | 0.779755 | 0.790676 | 0.777036 |\n",
    "| 800 | 0.036600 | 2.004199 | 0.783712 | 0.779772 | 0.778672 | 0.783712 |\n",
    "| 900 | 0.055700 | 2.261222 | 0.773031 | 0.770436 | 0.772057 | 0.773031 |\n",
    "| 1000 | 0.019700 | 2.102774 | 0.759680 | 0.763243 | 0.773193 | 0.759680 |\n",
    "| 1100 | 0.131900 | 2.729170 | 0.798398 | 0.788991 | 0.784868 | 0.798398 |\n",
    "| 1200 | 0.036700 | 2.929872 | 0.798398 | 0.790167 | 0.787715 | 0.798398 |\n",
    "| 1300 | 0.078500 | 3.012218 | 0.794393 | 0.788406 | 0.784523 | 0.794393 |\n",
    "| 1400 | 0.089700 | 3.154575 | 0.798398 | 0.790414 | 0.785774 | 0.798398 |\n",
    "| 1500 | 0.025700 | 3.104950 | 0.799733 | 0.793144 | 0.790542 | 0.799733 |\n",
    "| 1600 | 0.104200 | 3.259660 | 0.794393 | 0.786386 | 0.782162 | 0.794393 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693967f9",
   "metadata": {},
   "source": [
    "### Model Loading and Evaluation\n",
    "\n",
    "Load the best model and evaluate on validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc013060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the specified path\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"../models/indobert/final_model\")\n",
    "\n",
    "# Update the trainer to use the loaded model\n",
    "trainer.model = loaded_model\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde258de",
   "metadata": {},
   "source": [
    "#### Results Summary\n",
    "\n",
    "Display and compare evaluation metrics for validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf1d29ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| Metric           || Validation       || Test             ||\n",
      "||------------------||------------------||------------------||\n",
      "|| eval_accuracy    || 0.800            || 0.767            ||\n",
      "|| eval_f1          || 0.793            || 0.761            ||\n",
      "|| eval_precision   || 0.791            || 0.764            ||\n",
      "|| eval_recall      || 0.800            || 0.767            ||\n",
      "|| eval_loss        || 2.843            || 3.052            ||\n"
     ]
    }
   ],
   "source": [
    "def print_results_table(val_results, test_results):\n",
    "    metrics = ['eval_accuracy', 'eval_f1', 'eval_precision', 'eval_recall', 'eval_loss']\n",
    "    header = \"|| Metric           || Validation       || Test             ||\"\n",
    "    separator = \"||------------------||------------------||------------------||\"\n",
    "    rows = [\n",
    "        f\"|| {metric:<16} || {val_results.get(metric, 'N/A'):<16.3f} || {test_results.get(metric, 'N/A'):<16.3f} ||\"\n",
    "        if isinstance(val_results.get(metric), float) and isinstance(test_results.get(metric), float)\n",
    "        else f\"|| {metric:<16} || {val_results.get(metric, 'N/A'):<16} || {test_results.get(metric, 'N/A'):<16} ||\"\n",
    "        for metric in metrics\n",
    "    ]\n",
    "    print(\"\\n\".join([header, separator] + rows))\n",
    "\n",
    "print_results_table(val_results, test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "indonesian-tweet-classification-indobert-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
